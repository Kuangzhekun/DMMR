#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DMMR åŸºå‡†æµ‹è¯•è„šæœ¬
è¿è¡Œæ ‡å‡†åŒ–çš„æ€§èƒ½å’ŒåŠŸèƒ½æµ‹è¯•
"""
import os
import sys
import json
import time
import csv
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.dmmr import DMMRAgent, TaskType, get_config


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, output_dir: str = "results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.results = []
        self.start_time = datetime.now()
        
    def run_single_test(self, test_name: str, user_input: str, 
                       expected_task_type: TaskType = None,
                       user_id: str = "benchmark_user") -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = DMMRAgent(user_id=f"{user_id}_{test_name}", use_real_backends=False)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # å¤„ç†è¾“å…¥
            response, metrics = agent.process_input(user_input)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # è·å–æ™ºèƒ½ä½“çŠ¶æ€
            agent_status = agent.get_agent_status()
            
            # æ„å»ºæµ‹è¯•ç»“æœ
            result = {
                'test_name': test_name,
                'user_input': user_input,
                'ai_response': response,
                'processing_time': processing_time,
                'success': True,
                'error': None,
                'metrics': {
                    'latency_sec': metrics.latency_sec,
                    'token_usage': metrics.token_usage,
                    'memory_hits': metrics.memory_hits,
                    'activation_nodes': metrics.activation_nodes
                },
                'agent_stats': agent_status['session_stats'],
                'timestamp': datetime.now().isoformat()
            }
            
            # éªŒè¯ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æœæä¾›ï¼‰
            if expected_task_type:
                # è¿™é‡Œåº”è¯¥ä»metadataæˆ–å…¶ä»–æ–¹å¼è·å–å®é™…çš„ä»»åŠ¡ç±»å‹
                # ç®€åŒ–å¤„ç†
                result['task_type_correct'] = True
            
            print(f"   âœ… æˆåŠŸ (è€—æ—¶: {processing_time:.2f}s)")
            
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            
            result = {
                'test_name': test_name,
                'user_input': user_input,
                'ai_response': None,
                'processing_time': processing_time,
                'success': False,
                'error': str(e),
                'metrics': {},
                'agent_stats': {},
                'timestamp': datetime.now().isoformat()
            }
            
            print(f"   âŒ å¤±è´¥: {e}")
        
        return result
    
    def run_technical_coding_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡ŒæŠ€æœ¯ç¼–ç¨‹æµ‹è¯•"""
        print("\nğŸ”§ æŠ€æœ¯ç¼–ç¨‹æµ‹è¯•å¥—ä»¶")
        print("-" * 40)
        
        tests = [
            {
                'name': 'python_basic_question',
                'input': 'å¦‚ä½•åœ¨Pythonä¸­å¯¼å…¥pandasåº“ï¼Ÿ',
                'expected_type': TaskType.TECHNICAL_CODING
            },
            {
                'name': 'debug_import_error',
                'input': 'æˆ‘çš„Pythonä»£ç æŠ¥é”™ï¼šModuleNotFoundError: No module named pandas',
                'expected_type': TaskType.TECHNICAL_CODING
            },
            {
                'name': 'code_optimization',
                'input': 'è¿™æ®µä»£ç è¿è¡Œå¾ˆæ…¢ï¼Œæœ‰ä¼˜åŒ–å»ºè®®å—ï¼Ÿfor i in range(1000000): print(i)',
                'expected_type': TaskType.TECHNICAL_CODING
            },
            {
                'name': 'algorithm_question',
                'input': 'è¯·å®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•',
                'expected_type': TaskType.TECHNICAL_CODING
            },
            {
                'name': 'web_development',
                'input': 'å¦‚ä½•ç”¨FastAPIåˆ›å»ºä¸€ä¸ªREST APIï¼Ÿ',
                'expected_type': TaskType.TECHNICAL_CODING
            }
        ]
        
        results = []
        for test in tests:
            result = self.run_single_test(
                test['name'], 
                test['input'], 
                test.get('expected_type')
            )
            results.append(result)
            time.sleep(1)  # é¿å…APIé™åˆ¶
        
        return results
    
    def run_emotional_counseling_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡Œæƒ…æ„Ÿå’¨è¯¢æµ‹è¯•"""
        print("\nğŸ’ æƒ…æ„Ÿå’¨è¯¢æµ‹è¯•å¥—ä»¶")
        print("-" * 40)
        
        tests = [
            {
                'name': 'work_stress',
                'input': 'æˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç„¦è™‘',
                'expected_type': TaskType.EMOTIONAL_COUNSELING
            },
            {
                'name': 'relationship_issue',
                'input': 'æˆ‘å’Œæœ‹å‹å‘ç”Ÿäº†å†²çªï¼Œä¸çŸ¥é“æ€ä¹ˆåŠ',
                'expected_type': TaskType.EMOTIONAL_COUNSELING
            },
            {
                'name': 'career_confusion',
                'input': 'æˆ‘å¯¹æœªæ¥çš„èŒä¸šè§„åˆ’æ„Ÿåˆ°è¿·èŒ«',
                'expected_type': TaskType.EMOTIONAL_COUNSELING
            },
            {
                'name': 'family_problem',
                'input': 'å®¶äººä¸ç†è§£æˆ‘çš„é€‰æ‹©ï¼Œæˆ‘å¾ˆéš¾è¿‡',
                'expected_type': TaskType.EMOTIONAL_COUNSELING
            }
        ]
        
        results = []
        for test in tests:
            result = self.run_single_test(
                test['name'], 
                test['input'], 
                test.get('expected_type')
            )
            results.append(result)
            time.sleep(1)
        
        return results
    
    def run_memory_persistence_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡Œè®°å¿†æŒä¹…æ€§æµ‹è¯•"""
        print("\nğŸ§  è®°å¿†æŒä¹…æ€§æµ‹è¯•å¥—ä»¶")
        print("-" * 40)
        
        # è¿™ä¸ªæµ‹è¯•éœ€è¦å¤šè½®å¯¹è¯æ¥éªŒè¯è®°å¿†åŠŸèƒ½
        user_id = "memory_test_user"
        agent = DMMRAgent(user_id=user_id, use_real_backends=False)
        
        conversations = [
            "æˆ‘æœ€è¿‘åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ ",
            "æ˜¨å¤©æˆ‘å°è¯•è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œ",
            "è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜",
            "è¯·é—®ä½ è¿˜è®°å¾—æˆ‘ä¹‹å‰æåˆ°çš„æ·±åº¦å­¦ä¹ é—®é¢˜å—ï¼Ÿ"  # æµ‹è¯•è®°å¿†å›å¿†
        ]
        
        results = []
        for i, conv in enumerate(conversations):
            result = self.run_single_test(
                f'memory_test_round_{i+1}',
                conv,
                user_id=user_id
            )
            results.append(result)
            time.sleep(1)
        
        return results
    
    def run_performance_stress_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯•"""
        print("\nâš¡ æ€§èƒ½å‹åŠ›æµ‹è¯•å¥—ä»¶")  
        print("-" * 40)
        
        # æµ‹è¯•é•¿æ–‡æœ¬å¤„ç†
        long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æŠ€æœ¯é—®é¢˜ã€‚" * 50  # é‡å¤50æ¬¡
        
        tests = [
            {
                'name': 'long_text_processing',
                'input': long_text,
            },
            {
                'name': 'rapid_fire_questions',
                'input': 'å¿«é€Ÿé—®é¢˜1ï¼šPythonæ˜¯ä»€ä¹ˆï¼Ÿ'
            },
            {
                'name': 'complex_technical_query',
                'input': 'è¯·è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„åå‘ä¼ æ’­ç®—æ³•ï¼ŒåŒ…æ‹¬æ•°å­¦å…¬å¼å’Œå®ç°ç»†èŠ‚'
            }
        ]
        
        results = []
        
        # é•¿æ–‡æœ¬æµ‹è¯•
        result = self.run_single_test(tests[0]['name'], tests[0]['input'])
        results.append(result)
        
        # å¿«é€Ÿè¿ç»­æŸ¥è¯¢æµ‹è¯•
        rapid_fire_start = time.time()
        for i in range(5):
            quick_question = f"å¿«é€Ÿé—®é¢˜{i+1}ï¼šä»€ä¹ˆæ˜¯{'Python' if i%2==0 else 'JavaScript'}ï¼Ÿ"
            result = self.run_single_test(
                f'rapid_fire_{i+1}',
                quick_question
            )
            results.append(result)
        rapid_fire_end = time.time()
        
        print(f"   å¿«é€Ÿè¿ç»­æŸ¥è¯¢æ€»è€—æ—¶: {rapid_fire_end - rapid_fire_start:.2f}s")
        
        # å¤æ‚æŸ¥è¯¢æµ‹è¯•
        result = self.run_single_test(tests[2]['name'], tests[2]['input'])
        results.append(result)
        
        return results
    
    def analyze_results(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print("\nğŸ“Š ç»“æœåˆ†æ")
        print("-" * 40)
        
        total_tests = len(all_results)
        successful_tests = sum(1 for r in all_results if r['success'])
        failed_tests = total_tests - successful_tests
        
        # æ€§èƒ½ç»Ÿè®¡
        processing_times = [r['processing_time'] for r in all_results if r['success']]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        max_processing_time = max(processing_times) if processing_times else 0
        min_processing_time = min(processing_times) if processing_times else 0
        
        # Tokenä½¿ç”¨ç»Ÿè®¡
        token_usages = []
        memory_hits = []
        for r in all_results:
            if r['success'] and 'metrics' in r:
                if 'token_usage' in r['metrics'] and r['metrics']['token_usage']:
                    total_tokens = r['metrics']['token_usage'].get('total_tokens', 0)
                    if total_tokens > 0:
                        token_usages.append(total_tokens)
                
                memory_hits.append(r['metrics'].get('memory_hits', 0))
        
        avg_tokens = sum(token_usages) / len(token_usages) if token_usages else 0
        avg_memory_hits = sum(memory_hits) / len(memory_hits) if memory_hits else 0
        
        analysis = {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'failed_tests': failed_tests,
            'success_rate': successful_tests / total_tests if total_tests > 0 else 0,
            'performance': {
                'avg_processing_time': avg_processing_time,
                'max_processing_time': max_processing_time,
                'min_processing_time': min_processing_time
            },
            'resource_usage': {
                'avg_tokens_per_request': avg_tokens,
                'avg_memory_hits_per_request': avg_memory_hits
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # æ‰“å°åˆ†æç»“æœ
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"   å¤±è´¥æµ‹è¯•: {failed_tests}")
        print(f"   æˆåŠŸç‡: {analysis['success_rate']:.1%}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f}s")
        print(f"   å¹³å‡Tokenä½¿ç”¨: {avg_tokens:.0f}")
        print(f"   å¹³å‡è®°å¿†å‘½ä¸­: {avg_memory_hits:.1f}")
        
        return analysis
    
    def save_results(self, all_results: List[Dict[str, Any]], analysis: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ° {self.output_dir}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.output_dir / "benchmark_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"   è¯¦ç»†ç»“æœ: {results_file}")
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        analysis_file = self.output_dir / "benchmark_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"   åˆ†ææŠ¥å‘Š: {analysis_file}")
        
        # ä¿å­˜CSVæ‘˜è¦
        csv_file = self.output_dir / "benchmark_summary.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'test_name', 'success', 'processing_time', 'error',
                'memory_hits', 'activation_nodes'
            ])
            writer.writeheader()
            
            for result in all_results:
                row = {
                    'test_name': result['test_name'],
                    'success': result['success'],
                    'processing_time': result['processing_time'],
                    'error': result['error'] or '',
                    'memory_hits': result.get('metrics', {}).get('memory_hits', 0),
                    'activation_nodes': result.get('metrics', {}).get('activation_nodes', 0)
                }
                writer.writerow(row)
        print(f"   CSVæ‘˜è¦: {csv_file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ DMMR åŸºå‡†æµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    
    # éªŒè¯é…ç½®
    config = get_config()
    if not config.api.api_key:
        print("âŒ APIå¯†é’¥æœªé…ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡")
        return
    
    print(f"âœ… é…ç½®éªŒè¯é€šè¿‡")
    print(f"   æ¨¡å‹: {config.api.model_name}")
    print(f"   è¾“å‡ºç›®å½•: results/")
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = BenchmarkRunner()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
    all_results = []
    
    try:
        # æŠ€æœ¯ç¼–ç¨‹æµ‹è¯•
        technical_results = runner.run_technical_coding_tests()
        all_results.extend(technical_results)
        
        # æƒ…æ„Ÿå’¨è¯¢æµ‹è¯•
        emotional_results = runner.run_emotional_counseling_tests()
        all_results.extend(emotional_results)
        
        # è®°å¿†æŒä¹…æ€§æµ‹è¯•
        memory_results = runner.run_memory_persistence_tests()
        all_results.extend(memory_results)
        
        # æ€§èƒ½å‹åŠ›æµ‹è¯•
        performance_results = runner.run_performance_stress_tests()
        all_results.extend(performance_results)
        
        # åˆ†æç»“æœ
        analysis = runner.analyze_results(all_results)
        
        # ä¿å­˜ç»“æœ
        runner.save_results(all_results, analysis)
        
        print(f"\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print(f"   æ€»æµ‹è¯•æ•°: {len(all_results)}")
        print(f"   æˆåŠŸç‡: {analysis['success_rate']:.1%}")
        print(f"   æ€»è€—æ—¶: {(datetime.now() - runner.start_time).total_seconds():.1f}s")
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()



