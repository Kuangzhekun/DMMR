# -*- coding: utf-8 -*-
"""
é‡æ„å¼•æ“ - åŸºäºæ¿€æ´»è®°å¿†é‡æ„ç”ŸæˆAIå›ç­”
å®ç°è®°å¿†æ•´åˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›ç­”ç”Ÿæˆ
"""
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from .api_wrapper import APIWrapper


class ReconstructionEngine:
    """é‡æ„å¼•æ“ - å°†æ¿€æ´»çš„è®°å¿†ç‰‡æ®µæ•´åˆæˆè¿è´¯å›ç­”"""
    
    def __init__(self, api_wrapper: APIWrapper):
        self.api_wrapper = api_wrapper
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.reconstruction_count = 0
        self.total_memories_used = 0
        
        print("ğŸ”§ é‡æ„å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def reconstruct_answer(self, 
                          query: str,
                          retrieved_memories: List[Dict[str, Any]],
                          strategy_prompt: str = "",
                          code_only: bool = False,
                          generation_kwargs: Dict[str, Any] = None) -> Tuple[str, List[str], Dict[str, Any]]:
        """
        åŸºäºæ£€ç´¢è®°å¿†é‡æ„ç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_memories: æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
            strategy_prompt: è®¤çŸ¥ç­–ç•¥æç¤º
            code_only: æ˜¯å¦ä»…ç”Ÿæˆä»£ç 
            generation_kwargs: ç”Ÿæˆå‚æ•°
            
        Returns:
            (å›ç­”æ–‡æœ¬, ä½¿ç”¨çš„è®°å¿†IDåˆ—è¡¨, é‡æ„ç»Ÿè®¡)
        """
        print(f"ğŸ”§ å¼€å§‹é‡æ„å›ç­”...")
        print(f"   æŸ¥è¯¢: {query[:50]}{'...' if len(query) > 50 else ''}")
        print(f"   è®°å¿†æ•°é‡: {len(retrieved_memories)}")
        print(f"   ç­–ç•¥: {strategy_prompt[:30]}{'...' if len(strategy_prompt) > 30 else ''}")
        
        # å‚æ•°å¤„ç†
        generation_kwargs = generation_kwargs or {}
        max_context_items = generation_kwargs.get('max_context_items', 5)
        max_context_chars = generation_kwargs.get('max_context_chars', 200)
        
        # è®°å¿†ä¸Šä¸‹æ–‡å¤„ç†
        processed_memories = self._process_memories(
            retrieved_memories, 
            max_context_items, 
            max_context_chars
        )
        
        memory_context = self._prepare_memory_context(processed_memories)
        used_memory_ids = [mem['id'] for mem in processed_memories if 'id' in mem]
        
        # æ„å»ºé‡æ„æç¤º
        full_prompt = self._build_reconstruction_prompt(
            query=query,
            memory_context=memory_context,
            strategy_prompt=strategy_prompt,
            code_only=code_only
        )
        
        # ç”Ÿæˆå›ç­”
        try:
            answer = self.api_wrapper.generate_text(
                full_prompt, 
                **generation_kwargs
            )
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨æ›´ç®€å•çš„æç¤º
            fallback_prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\nè¯·ç®€è¦å›ç­”ï¼š"
            answer = self.api_wrapper.generate_text(fallback_prompt, max_tokens=500)
        
        # åå¤„ç†
        if code_only:
            answer = self._clean_code_response(answer)
        else:
            answer = self._enhance_response(answer, strategy_prompt)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = self._calculate_reconstruction_stats(
            processed_memories, full_prompt, answer
        )
        
        # æ›´æ–°å†…éƒ¨ç»Ÿè®¡
        self.reconstruction_count += 1
        self.total_memories_used += len(processed_memories)
        
        print(f"âœ… é‡æ„å®Œæˆ (ä½¿ç”¨è®°å¿†: {len(processed_memories)}, å›ç­”é•¿åº¦: {len(answer)})")
        
        return answer, used_memory_ids, stats
    
    def _process_memories(self, memories: List[Dict[str, Any]], 
                         max_items: int, max_chars: int) -> List[Dict[str, Any]]:
        """å¤„ç†å’Œè¿‡æ»¤è®°å¿†"""
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_memories = sorted(
            memories,
            key=lambda m: m.get('significance_score', 0.0),
            reverse=True
        )
        
        # é™åˆ¶æ•°é‡
        limited_memories = sorted_memories[:max_items]
        
        # æˆªæ–­å†…å®¹
        processed = []
        for memory in limited_memories:
            processed_memory = dict(memory)
            content = processed_memory.get('content', '')
            
            if len(content) > max_chars:
                processed_memory['content'] = content[:max_chars] + '...'
            
            processed.append(processed_memory)
        
        return processed
    
    def _prepare_memory_context(self, memories: List[Dict[str, Any]]) -> str:
        """å‡†å¤‡è®°å¿†ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not memories:
            return "ï¼ˆæ²¡æœ‰ç›¸å…³çš„å†å²è®°å¿†ï¼‰"
        
        context_parts = []
        
        for i, memory in enumerate(memories, 1):
            content = memory.get('content', 'æœªçŸ¥å†…å®¹')
            score = memory.get('significance_score', 0.0)
            source = memory.get('source', 'è®°å¿†')
            
            # æ ¼å¼åŒ–æ—¶é—´ä¿¡æ¯
            timestamp = memory.get('timestamp')
            time_str = self._format_timestamp(timestamp)
            
            # æ„å»ºè®°å¿†æ¡ç›®
            context_part = (
                f"{source}{i} ({time_str}, é‡è¦æ€§:{score:.2f}): {content}"
            )
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def _format_timestamp(self, timestamp: Any) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        if not timestamp:
            return "æœ€è¿‘"
        
        try:
            if isinstance(timestamp, str):
                # å°è¯•è§£æISOæ ¼å¼
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            elif isinstance(timestamp, datetime):
                dt = timestamp
            else:
                return "æœ€è¿‘"
            
            return dt.strftime('%m-%d %H:%M')
        except:
            return "æœ€è¿‘"
    
    def _build_reconstruction_prompt(self, query: str, memory_context: str,
                                   strategy_prompt: str, code_only: bool) -> str:
        """æ„å»ºé‡æ„æç¤º"""
        if code_only:
            return self._build_code_generation_prompt(query, memory_context)
        else:
            return self._build_conversational_prompt(query, memory_context, strategy_prompt)
    
    def _build_code_generation_prompt(self, query: str, memory_context: str) -> str:
        """æ„å»ºä»£ç ç”Ÿæˆæç¤º"""
        return f"""
åŸºäºä»¥ä¸‹å†å²è®°å¿†ä¸­çš„æŠ€æœ¯ä¿¡æ¯ï¼Œç”Ÿæˆè§£å†³ç”¨æˆ·é—®é¢˜çš„Pythonä»£ç ã€‚

=== æŠ€æœ¯è®°å¿†å‚è€ƒ ===
{memory_context}

=== ç”¨æˆ·éœ€æ±‚ ===
{query}

=== ä»£ç å®ç° ===
è¯·æä¾›å®Œæ•´ã€å¯æ‰§è¡Œçš„Pythonä»£ç ï¼ŒåŒ…å«é€‚å½“çš„ç±»å‹æ³¨è§£å’Œé”™è¯¯å¤„ç†ï¼š

```python
"""
    
    def _build_conversational_prompt(self, query: str, memory_context: str, strategy_prompt: str) -> str:
        """æ„å»ºå¯¹è¯å¼å›ç­”æç¤º"""
        base_instruction = (
            "è¯·åŸºäºä»¥ä¸‹å†å²è®°å¿†å’Œç”¨æˆ·å½“å‰çš„é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæœ‰å¸®åŠ©ã€å‡†ç¡®çš„å›ç­”ã€‚"
            "è‡ªç„¶åœ°èå…¥ç›¸å…³çš„å†å²ä¿¡æ¯ï¼Œä¿æŒå›ç­”çš„è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ã€‚"
        )
        
        if strategy_prompt:
            instruction = f"{strategy_prompt}\n\n{base_instruction}"
        else:
            instruction = base_instruction
        
        return f"""
{instruction}

=== ç›¸å…³å†å²è®°å¿† ===
{memory_context}

=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{query}

=== æ‚¨çš„å›ç­” ===
"""
    
    def _clean_code_response(self, response: str) -> str:
        """æ¸…ç†ä»£ç å“åº”"""
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        if "```python" in response:
            start = response.find("```python") + 9
            end = response.find("```", start)
            if end != -1:
                response = response[start:end].strip()
        elif "```" in response:
            # å¤„ç†å…¶ä»–ä»£ç å—
            start = response.find("```") + 3
            end = response.find("```", start)
            if end != -1:
                response = response[start:end].strip()
        
        return response
    
    def _enhance_response(self, response: str, strategy_prompt: str) -> str:
        """å¢å¼ºå›ç­”"""
        # åŸºäºç­–ç•¥æç¤ºè¿›è¡Œåå¤„ç†
        if "å…±æƒ…" in strategy_prompt or "emotional" in strategy_prompt.lower():
            response = self._add_empathy(response)
        elif "æŠ€æœ¯" in strategy_prompt or "technical" in strategy_prompt.lower():
            response = self._add_technical_formatting(response)
        
        return response
    
    def _add_empathy(self, response: str) -> str:
        """æ·»åŠ å…±æƒ…å…ƒç´ """
        # ç®€å•çš„å…±æƒ…å¢å¼º
        empathy_starters = [
            "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œ",
            "è¿™ç¡®å®å¯èƒ½è®©äººæ„Ÿåˆ°å›°æ‰°ï¼Œ",
            "æˆ‘èƒ½ä½“ä¼šåˆ°æ‚¨çš„å¤„å¢ƒï¼Œ"
        ]
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å…±æƒ…è¡¨è¾¾
        if not any(starter in response for starter in empathy_starters):
            # ç®€å•æ·»åŠ å…±æƒ…å‰ç¼€
            if len(response) > 0:
                response = f"æˆ‘ç†è§£æ‚¨çš„æƒ…å†µã€‚{response}"
        
        return response
    
    def _add_technical_formatting(self, response: str) -> str:
        """æ·»åŠ æŠ€æœ¯æ ¼å¼åŒ–"""
        # ç®€å•çš„æŠ€æœ¯å›ç­”æ ¼å¼åŒ–
        if any(keyword in response for keyword in ['ä»£ç ', 'def', 'class', 'import']):
            if not response.startswith("æŠ€æœ¯è§£ç­”ï¼š"):
                response = f"æŠ€æœ¯è§£ç­”ï¼š\n\n{response}"
            
            if not response.endswith("å¦‚æœ‰å…¶ä»–æŠ€æœ¯é—®é¢˜ï¼Œè¯·éšæ—¶è¯¢é—®ã€‚"):
                response = f"{response}\n\nå¦‚æœ‰å…¶ä»–æŠ€æœ¯é—®é¢˜ï¼Œè¯·éšæ—¶è¯¢é—®ã€‚"
        
        return response
    
    def _calculate_reconstruction_stats(self, memories: List[Dict], prompt: str, answer: str) -> Dict[str, Any]:
        """è®¡ç®—é‡æ„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'memories_used': len(memories),
            'prompt_length': len(prompt),
            'answer_length': len(answer),
            'avg_memory_score': (
                sum(m.get('significance_score', 0.0) for m in memories) / len(memories)
                if memories else 0.0
            ),
            'reconstruction_timestamp': datetime.now().isoformat()
        }
    
    def get_reconstruction_summary(self) -> Dict[str, Any]:
        """è·å–é‡æ„å¼•æ“è¿è¡Œæ‘˜è¦"""
        return {
            'total_reconstructions': self.reconstruction_count,
            'total_memories_used': self.total_memories_used,
            'avg_memories_per_reconstruction': (
                self.total_memories_used / max(self.reconstruction_count, 1)
            )
        }


