# -*- coding: utf-8 -*-
"""
æ¿€æ´»å¼•æ“ - å®ç°è®¤çŸ¥å¯åŠ¨å’Œæ‰©æ•£æ¿€æ´»
åŸºäºACT-Rç†è®ºçš„è”æƒ³è®°å¿†æ¿€æ´»æœºåˆ¶
"""
import asyncio
import math
from typing import List, Dict, Any, Tuple, Optional, Set
from .data_models import Node, TaskType
from .memory_systems import MultipleMemorySystems
from .config import get_config


class ActivationCache:
    """é«˜é€Ÿæ¿€æ´»ç¼“å­˜ï¼Œæ¨¡æ‹Ÿè®¤çŸ¥å°±ç»ªçŠ¶æ€"""
    
    def __init__(self, max_size: int = 100):
        self.cache: Dict[str, Any] = {}
        self.max_size = max_size
        print(f"âš¡ æ¿€æ´»ç¼“å­˜åˆå§‹åŒ– (å®¹é‡: {max_size})")
    
    async def get(self, key: str) -> Optional[Any]:
        """å¼‚æ­¥è·å–ç¼“å­˜é¡¹"""
        return self.cache.get(key)
    
    async def set(self, key: str, value: Any):
        """å¼‚æ­¥è®¾ç½®ç¼“å­˜é¡¹"""
        if len(self.cache) >= self.max_size:
            await self.evict()
        self.cache[key] = value
    
    async def evict(self):
        """ç¼“å­˜æ·˜æ±°ç­–ç•¥"""
        # ç®€å•å®ç°ï¼šåˆ é™¤å‰20%çš„é¡¹ç›®
        evict_count = int(self.max_size * 0.2)
        keys_to_remove = list(self.cache.keys())[:evict_count]
        
        for key in keys_to_remove:
            del self.cache[key]
        
        print(f"  [ç¼“å­˜æ¸…ç†] æ·˜æ±°äº† {evict_count} ä¸ªç¼“å­˜é¡¹")


class ActivationEngine:
    """æ¿€æ´»å¼•æ“ - DMMRçš„æ ¸å¿ƒè®¤çŸ¥æœºåˆ¶"""
    
    def __init__(self, memory: MultipleMemorySystems):
        self.memory = memory
        self.cache = ActivationCache()
        self.config = get_config().activation
        
        # æ¿€æ´»å‚æ•°
        self.decay_factor = self.config.decay_factor
        self.activation_threshold = self.config.activation_threshold
        self.fan_out_factor = self.config.fan_out_factor
        self.max_depth = self.config.max_depth
        
        # æ¿€æ´»çŠ¶æ€è·Ÿè¸ª
        self.active_nodes: Dict[str, float] = {}
        
        # é¢„å–ç»Ÿè®¡
        self.prefetch_stats = {
            "total_prefetches": 0,
            "useful_prefetches": 0
        }
        self._last_prefetch_ids: Set[str] = set()
        self._last_turn_stats = {"total": 0, "useful": 0}
        
        print(f"âš¡ æ¿€æ´»å¼•æ“åˆå§‹åŒ–å®Œæˆ (é˜ˆå€¼: {self.activation_threshold}, è¡°å‡: {self.decay_factor})")
    
    async def cognitive_priming(self, user_id: str, context_hints: List[str] = None):
        """
        è®¤çŸ¥å¯åŠ¨ - é¢„åŠ è½½ç›¸å…³è®°å¿†åˆ°è®¤çŸ¥å°±ç»ªçŠ¶æ€
        æ¨¡æ‹Ÿè®°å¿†ç³»ç»Ÿçš„é¢„æ¿€æ´»è¿‡ç¨‹
        """
        print(f"ğŸ§  å¼€å§‹è®¤çŸ¥å¯åŠ¨ (ç”¨æˆ·: {user_id})")
        
        # é¢„å–é«˜é¢‘èŠ‚ç‚¹
        high_priority_nodes = ["Python", "Bug", "PowerShell", "API", "å­¦ä¹ ", "æœ‹å‹"]
        
        prefetch_count = 0
        for node_id in high_priority_nodes:
            # å°è¯•ä»è¯­ä¹‰è®°å¿†è·å–èŠ‚ç‚¹
            node = self.memory.get_semantic_node(node_id)
            if node:
                await self.cache.set(f"node:{node_id}", node)
                self._last_prefetch_ids.add(node_id)
                prefetch_count += 1
                print(f"  â†’ é¢„å–èŠ‚ç‚¹: {node_id}")
            
            # ä¹Ÿä»ç¨‹åºè®°å¿†å°è¯•è·å–
            proc_node = self.memory.get_procedural_node(node_id)
            if proc_node:
                await self.cache.set(f"proc_node:{node_id}", proc_node)
                prefetch_count += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self._last_turn_stats["total"] = prefetch_count
        self.prefetch_stats["total_prefetches"] += prefetch_count
        
        # å¯åŠ¨åå°è¯­ä¹‰æ‰©å±•
        asyncio.create_task(self._background_semantic_expansion(user_id))
        
        print(f"âœ… è®¤çŸ¥å¯åŠ¨å®Œæˆ (é¢„å–: {prefetch_count} ä¸ªèŠ‚ç‚¹)")
    
    async def _background_semantic_expansion(self, user_id: str):
        """åå°è¯­ä¹‰æ‰©å±•ä»»åŠ¡"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†å»¶è¿Ÿ
        
        print("  [åå°] è¯­ä¹‰æ‰©å±•ä»»åŠ¡æ‰§è¡Œä¸­...")
        
        # æ¨¡æ‹Ÿä»è®°å¿†ç³»ç»Ÿä¸­å‘ç°æ›´å¤šç›¸å…³èŠ‚ç‚¹
        expansion_nodes = ["Docker", "JavaScript", "Framework", "Exception"]
        
        for node_id in expansion_nodes:
            node = self.memory.get_semantic_node(node_id)
            if node:
                await self.cache.set(f"expanded:{node_id}", node)
        
        print("  [åå°] è¯­ä¹‰æ‰©å±•å®Œæˆ")
    
    def spreading_activation(self, cues: List[Tuple[str, float]], 
                           task_type: TaskType, 
                           max_depth: Optional[int] = None) -> List[Node]:
        """
        æ‰©æ•£æ¿€æ´» - æ ¸å¿ƒè”æƒ³è®°å¿†ç®—æ³•
        åŸºäºACT-Rç†è®ºå®ç°æ¿€æ´»èƒ½é‡åœ¨è®°å¿†ç½‘ç»œä¸­çš„ä¼ æ’­
        
        Args:
            cues: æ¿€æ´»çº¿ç´¢åˆ—è¡¨ [(èŠ‚ç‚¹ID, åˆå§‹èƒ½é‡)]
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå½±å“æ³¨æ„åŠ›æƒé‡
            max_depth: æœ€å¤§æ‰©æ•£æ·±åº¦
            
        Returns:
            æ¿€æ´»çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        max_depth = max_depth or self.max_depth
        print(f"ğŸ”¥ å¼€å§‹æ‰©æ•£æ¿€æ´» (çº¿ç´¢: {len(cues)}, ç±»å‹: {task_type.value}, æ·±åº¦: {max_depth})")
        
        # é‡ç½®æ¿€æ´»çŠ¶æ€
        self.active_nodes.clear()
        
        # è·å–ä»»åŠ¡ç›¸å…³çš„æ³¨æ„åŠ›æƒé‡
        attention_weights = self._get_attention_weights(task_type)
        
        # åˆå§‹åŒ–æ¿€æ´»é˜Ÿåˆ—
        activation_queue = []
        for node_id, initial_energy in cues:
            self.active_nodes[node_id] = initial_energy
            activation_queue.append((node_id, 0))  # (èŠ‚ç‚¹ID, æ·±åº¦)
        
        visited = set()
        
        # BFSæ‰©æ•£æ¿€æ´»
        while activation_queue:
            current_id, depth = activation_queue.pop(0)
            
            if depth >= max_depth or current_id in visited:
                continue
            
            visited.add(current_id)
            current_activation = self.active_nodes.get(current_id, 0)
            
            # æ£€æŸ¥æ¿€æ´»é˜ˆå€¼
            if current_activation < self.activation_threshold:
                continue
            
            print(f"  â†’ å¤„ç†èŠ‚ç‚¹: {current_id} (æ¿€æ´»: {current_activation:.3f}, æ·±åº¦: {depth})")
            
            # è·å–é‚»å±…èŠ‚ç‚¹
            neighbors = self._get_all_neighbors(current_id)
            
            # ä¼ æ’­æ¿€æ´»èƒ½é‡
            for neighbor_node, edge_weight in neighbors:
                # è®¡ç®—æ³¨æ„åŠ›æƒé‡
                attention_weight = attention_weights.get(neighbor_node.label, 1.0)
                
                # ACT-Ræ¿€æ´»ä¼ æ’­å…¬å¼ï¼šA_i = B_i + Î£(W_j * S_ji)
                # è¿™é‡Œç®€åŒ–ä¸ºï¼šreceived_energy = sender_activation * decay * edge_weight * attention_weight
                received_energy = (current_activation * 
                                 self.decay_factor * 
                                 edge_weight * 
                                 attention_weight)
                
                # æ›´æ–°é‚»å±…èŠ‚ç‚¹æ¿€æ´»å€¼
                neighbor_id = neighbor_node.id
                self.active_nodes[neighbor_id] = (
                    self.active_nodes.get(neighbor_id, 0) + received_energy
                )
                
                # æ·»åŠ åˆ°æ¿€æ´»é˜Ÿåˆ—
                if neighbor_id not in visited:
                    activation_queue.append((neighbor_id, depth + 1))
            
            # è·¨æ¨¡æ€æ¿€æ´»é—¨æ§
            if current_activation > 0.8:  # é«˜æ¿€æ´»é˜ˆå€¼è§¦å‘è·¨æ¨¡æ€
                await self._cross_modal_activation(current_id)
        
        # æ”¶é›†å¹¶è¿”å›æ¿€æ´»èŠ‚ç‚¹
        activated_nodes = self._collect_activated_nodes()
        
        print(f"âœ… æ‰©æ•£æ¿€æ´»å®Œæˆ (æ¿€æ´»èŠ‚ç‚¹: {len(activated_nodes)})")
        return activated_nodes
    
    def _get_attention_weights(self, task_type: TaskType) -> Dict[str, float]:
        """è·å–ä»»åŠ¡ç›¸å…³çš„æ³¨æ„åŠ›æƒé‡"""
        attention_maps = {
            TaskType.TECHNICAL_CODING: {
                "Technology": 1.5,
                "Concept": 1.2, 
                "Problem": 1.3,
                "default": 0.8
            },
            TaskType.EMOTIONAL_COUNSELING: {
                "Person": 1.5,
                "Goal": 1.3,
                "Activity": 1.2,
                "default": 0.7
            },
            TaskType.CREATIVE_WRITING: {
                "Concept": 1.4,
                "Activity": 1.2,
                "default": 1.0
            },
            TaskType.EDUCATIONAL: {
                "Concept": 1.5,
                "Technology": 1.2,
                "default": 1.0
            }
        }
        
        return attention_maps.get(task_type, {"default": 1.0})
    
    def _get_all_neighbors(self, node_id: str) -> List[Tuple[Node, float]]:
        """è·å–èŠ‚ç‚¹çš„æ‰€æœ‰é‚»å±…ï¼ˆè¯­ä¹‰+ç¨‹åºè®°å¿†ï¼‰"""
        neighbors = []
        
        # è¯­ä¹‰è®°å¿†é‚»å±…
        semantic_neighbors = self.memory.get_semantic_weighted_neighbors(node_id)
        neighbors.extend(semantic_neighbors)
        
        # ç¨‹åºè®°å¿†é‚»å±…
        procedural_neighbors = self.memory.get_procedural_weighted_neighbors(node_id)
        neighbors.extend(procedural_neighbors)
        
        return neighbors
    
    async def _cross_modal_activation(self, node_id: str):
        """è·¨æ¨¡æ€æ¿€æ´» - ä»å›¾è®°å¿†è§¦å‘å‘é‡è®°å¿†æ£€ç´¢"""
        print(f"  ğŸ”— è·¨æ¨¡æ€æ¿€æ´»: {node_id}")
        
        # è·å–èŠ‚ç‚¹
        node = (self.memory.get_semantic_node(node_id) or 
                self.memory.get_procedural_node(node_id))
        
        if node and node.embedding:
            # åœ¨æƒ…æ™¯è®°å¿†ä¸­æœç´¢ç›¸å…³äº‹ä»¶
            episodic_memories = self.memory.search_episodic_by_vector(
                node.embedding, n_results=2
            )
            
            if episodic_memories:
                print(f"    â†’ å‘ç°ç›¸å…³æƒ…æ™¯è®°å¿†: {len(episodic_memories)} ä¸ª")
                
                # å¯ä»¥è¿›ä¸€æ­¥å°†æƒ…æ™¯è®°å¿†ä¸­çš„å®ä½“é‡æ–°æ³¨å…¥æ¿€æ´»ç½‘ç»œ
                # è¿™é‡Œç®€åŒ–å¤„ç†
    
    def _collect_activated_nodes(self) -> List[Node]:
        """æ”¶é›†æ‰€æœ‰æ¿€æ´»èŠ‚ç‚¹"""
        activated_nodes = []
        
        for node_id, activation_energy in self.active_nodes.items():
            if activation_energy > self.activation_threshold:
                # å°è¯•ä»ä¸¤ä¸ªè®°å¿†ç³»ç»Ÿè·å–èŠ‚ç‚¹
                node = (self.memory.get_semantic_node(node_id) or 
                       self.memory.get_procedural_node(node_id))
                
                if node:
                    # å°†æ¿€æ´»èƒ½é‡å­˜å‚¨åœ¨èŠ‚ç‚¹å±æ€§ä¸­
                    node.properties["activation"] = activation_energy
                    activated_nodes.append(node)
        
        # æŒ‰æ¿€æ´»èƒ½é‡æ’åº
        activated_nodes.sort(
            key=lambda n: n.properties.get("activation", 0),
            reverse=True
        )
        
        return activated_nodes
    
    def mark_useful_prefetch(self, used_memory_ids: List[str]):
        """æ ‡è®°æœ‰ç”¨çš„é¢„å–é¡¹"""
        if not used_memory_ids:
            return
        
        used_set = set(used_memory_ids)
        hits = len(self._last_prefetch_ids.intersection(used_set))
        
        if hits > 0:
            self._last_turn_stats["useful"] += hits
            self.prefetch_stats["useful_prefetches"] += hits
            print(f"  ğŸ“Š é¢„å–å‘½ä¸­: {hits}/{len(self._last_prefetch_ids)}")
    
    def reward_activation_path(self, path_nodes: List[str], reward: float = 0.1):
        """å¥–åŠ±æˆåŠŸçš„æ¿€æ´»è·¯å¾„ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰"""
        print(f"  ğŸ¯ è·¯å¾„å¥–åŠ±: {len(path_nodes)} ä¸ªèŠ‚ç‚¹, å¥–åŠ±: {reward}")
        
        # åœ¨çœŸå®ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å›¾ä¸­è¾¹çš„æƒé‡
        # ç®€åŒ–å®ç°ï¼šè®°å½•æˆåŠŸè·¯å¾„ç”¨äºæœªæ¥ä¼˜åŒ–
        for i, node_id in enumerate(path_nodes):
            if node_id in self.active_nodes:
                # ç•¥å¾®å¢åŠ è¯¥èŠ‚ç‚¹çš„åŸºç¡€æ¿€æ´»æ°´å¹³
                self.active_nodes[node_id] += reward * (1.0 - i * 0.1)
    
    def get_prefetch_stats(self) -> Dict[str, int]:
        """è·å–é¢„å–ç»Ÿè®¡ä¿¡æ¯"""
        return dict(self.prefetch_stats)
    
    def get_and_reset_prefetch_stats(self) -> Dict[str, int]:
        """è·å–å¹¶é‡ç½®å•è½®ç»Ÿè®¡"""
        stats = dict(self._last_turn_stats)
        
        # é‡ç½®å•è½®ç»Ÿè®¡
        self._last_turn_stats = {"total": 0, "useful": 0}
        self._last_prefetch_ids.clear()
        
        return stats
    
    def get_activation_summary(self) -> Dict[str, Any]:
        """è·å–æ¿€æ´»å¼•æ“è¿è¡Œæ‘˜è¦"""
        return {
            "active_nodes_count": len(self.active_nodes),
            "total_activation_energy": sum(self.active_nodes.values()),
            "avg_activation_energy": (
                sum(self.active_nodes.values()) / len(self.active_nodes)
                if self.active_nodes else 0
            ),
            "cache_size": len(self.cache.cache),
            "prefetch_hit_rate": (
                self.prefetch_stats["useful_prefetches"] / 
                max(self.prefetch_stats["total_prefetches"], 1)
            )
        }



