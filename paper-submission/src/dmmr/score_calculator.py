# -*- coding: utf-8 -*-
"""
è¯„åˆ†è®¡ç®—å™¨ - è®¡ç®—è®°å¿†é‡è¦æ€§å’Œç›¸å…³æ€§åˆ†æ•°
åŸºäºå¤šç§å¯å‘å¼æ–¹æ³•è¯„ä¼°è®°å¿†çš„ä»·å€¼
"""
import math
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from .data_models import MemoryChunk, TaskType


class ScoreCalculator:
    """è®°å¿†é‡è¦æ€§è¯„åˆ†è®¡ç®—å™¨"""
    
    def __init__(self):
        # è¯„åˆ†æƒé‡é…ç½®
        self.weights = {
            'recency': 0.3,      # æ—¶é—´æ–°è¿‘æ€§
            'frequency': 0.25,   # è®¿é—®é¢‘ç‡
            'relevance': 0.2,    # å†…å®¹ç›¸å…³æ€§
            'task_match': 0.15,  # ä»»åŠ¡ç±»å‹åŒ¹é…
            'user_feedback': 0.1 # ç”¨æˆ·åé¦ˆ
        }
        
        # ä»»åŠ¡ç±»å‹æƒé‡
        self.task_type_weights = {
            TaskType.TECHNICAL_CODING: 1.2,
            TaskType.EMOTIONAL_COUNSELING: 1.1,
            TaskType.CREATIVE_WRITING: 1.0,
            TaskType.EDUCATIONAL: 1.1,
            TaskType.GENERAL_QA: 1.0
        }
        
        print("ğŸ“Š è¯„åˆ†è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def calculate_initial_score(self, chunk: MemoryChunk) -> float:
        """
        è®¡ç®—è®°å¿†å—çš„åˆå§‹é‡è¦æ€§åˆ†æ•°
        
        Args:
            chunk: è®°å¿†å—
            
        Returns:
            é‡è¦æ€§åˆ†æ•° (0.0 - 1.0)
        """
        scores = {
            'recency': self._calculate_recency_score(chunk),
            'content_quality': self._calculate_content_quality_score(chunk),
            'task_importance': self._calculate_task_importance_score(chunk),
            'metadata_richness': self._calculate_metadata_score(chunk)
        }
        
        # åŠ æƒå¹³å‡
        total_score = (
            scores['recency'] * 0.3 +
            scores['content_quality'] * 0.4 +
            scores['task_importance'] * 0.2 +
            scores['metadata_richness'] * 0.1
        )
        
        # ç¡®ä¿åˆ†æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
        final_score = max(0.0, min(1.0, total_score))
        
        return final_score
    
    def calculate_relevance_score(self, chunk: MemoryChunk, 
                                query: str, 
                                task_type: TaskType) -> float:
        """
        è®¡ç®—è®°å¿†ä¸å½“å‰æŸ¥è¯¢çš„ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            chunk: è®°å¿†å—
            query: æŸ¥è¯¢æ–‡æœ¬
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            ç›¸å…³æ€§åˆ†æ•° (0.0 - 1.0)
        """
        scores = {
            'text_similarity': self._calculate_text_similarity(chunk.content, query),
            'task_match': self._calculate_task_match_score(chunk.task_type, task_type),
            'keyword_overlap': self._calculate_keyword_overlap(chunk, query),
            'context_relevance': self._calculate_context_relevance(chunk, query)
        }
        
        # åŠ æƒè®¡ç®—æ€»åˆ†
        relevance_score = (
            scores['text_similarity'] * 0.4 +
            scores['task_match'] * 0.3 +
            scores['keyword_overlap'] * 0.2 +
            scores['context_relevance'] * 0.1
        )
        
        return max(0.0, min(1.0, relevance_score))
    
    def update_score_with_feedback(self, chunk: MemoryChunk, 
                                  feedback_type: str, 
                                  feedback_value: float) -> float:
        """
        åŸºäºç”¨æˆ·åé¦ˆæ›´æ–°è®°å¿†åˆ†æ•°
        
        Args:
            chunk: è®°å¿†å—
            feedback_type: åé¦ˆç±»å‹ ('useful', 'not_useful', 'accurate', 'inaccurate')
            feedback_value: åé¦ˆå€¼ (-1.0 åˆ° 1.0)
            
        Returns:
            æ›´æ–°åçš„åˆ†æ•°
        """
        current_score = chunk.significance_score
        
        # åé¦ˆå½±å“æƒé‡
        feedback_weights = {
            'useful': 0.2,
            'not_useful': -0.2,
            'accurate': 0.15,
            'inaccurate': -0.25
        }
        
        feedback_impact = feedback_weights.get(feedback_type, 0.0) * abs(feedback_value)
        new_score = current_score + feedback_impact
        
        # é™åˆ¶åˆ†æ•°èŒƒå›´
        new_score = max(0.0, min(1.0, new_score))
        
        return new_score
    
    def _calculate_recency_score(self, chunk: MemoryChunk) -> float:
        """è®¡ç®—æ—¶é—´æ–°è¿‘æ€§åˆ†æ•°"""
        try:
            if isinstance(chunk.timestamp, str):
                timestamp = datetime.fromisoformat(chunk.timestamp.replace('Z', '+00:00'))
            else:
                timestamp = chunk.timestamp
            
            # è®¡ç®—æ—¶é—´å·®ï¼ˆå°æ—¶ï¼‰
            time_diff = (datetime.now() - timestamp.replace(tzinfo=None)).total_seconds() / 3600
            
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°
            # 24å°æ—¶å†…å¾—åˆ†ä¸º1.0ï¼Œä¹‹åæŒ‡æ•°è¡°å‡
            decay_rate = 0.1  # è¡°å‡ç‡
            recency_score = math.exp(-decay_rate * time_diff / 24)
            
            return max(0.0, min(1.0, recency_score))
            
        except Exception:
            # è§£æå¤±è´¥æ—¶è¿”å›ä¸­ç­‰åˆ†æ•°
            return 0.5
    
    def _calculate_content_quality_score(self, chunk: MemoryChunk) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        content = chunk.content
        if not content:
            return 0.0
        
        quality_indicators = {
            'length': min(len(content) / 100, 1.0),  # å†…å®¹é•¿åº¦
            'structure': self._assess_content_structure(content),
            'informativeness': self._assess_informativeness(content),
            'clarity': self._assess_clarity(content)
        }
        
        # åŠ æƒå¹³å‡
        quality_score = (
            quality_indicators['length'] * 0.2 +
            quality_indicators['structure'] * 0.3 +
            quality_indicators['informativeness'] * 0.3 +
            quality_indicators['clarity'] * 0.2
        )
        
        return quality_score
    
    def _calculate_task_importance_score(self, chunk: MemoryChunk) -> float:
        """è®¡ç®—ä»»åŠ¡é‡è¦æ€§åˆ†æ•°"""
        task_type = chunk.task_type
        base_weight = self.task_type_weights.get(task_type, 1.0)
        
        # æ ¹æ®ä»»åŠ¡ç‰¹å®šç‰¹å¾è°ƒæ•´
        content_lower = chunk.content.lower()
        
        if task_type == TaskType.TECHNICAL_CODING:
            # æŠ€æœ¯å†…å®¹çš„é‡è¦æ€§æŒ‡æ ‡
            tech_indicators = ['bug', 'é”™è¯¯', 'exception', 'é—®é¢˜', 'è§£å†³æ–¹æ¡ˆ']
            importance_boost = sum(1 for indicator in tech_indicators if indicator in content_lower) * 0.1
        elif task_type == TaskType.EMOTIONAL_COUNSELING:
            # æƒ…æ„Ÿå†…å®¹çš„é‡è¦æ€§æŒ‡æ ‡
            emotion_indicators = ['æ„Ÿè§‰', 'æƒ…ç»ª', 'å‹åŠ›', 'ç„¦è™‘', 'å›°éš¾', 'æ”¯æŒ']
            importance_boost = sum(1 for indicator in emotion_indicators if indicator in content_lower) * 0.1
        else:
            importance_boost = 0.0
        
        return min(1.0, base_weight * 0.5 + importance_boost)
    
    def _calculate_metadata_score(self, chunk: MemoryChunk) -> float:
        """è®¡ç®—å…ƒæ•°æ®ä¸°å¯Œåº¦åˆ†æ•°"""
        metadata = chunk.metadata
        if not metadata:
            return 0.3
        
        richness_indicators = [
            'user_id' in metadata,
            'task_type' in metadata,
            'keywords' in metadata,
            'context' in metadata,
            'importance' in metadata
        ]
        
        richness_score = sum(richness_indicators) / len(richness_indicators)
        return richness_score
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # åˆ†è¯
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_task_match_score(self, chunk_task: TaskType, query_task: TaskType) -> float:
        """è®¡ç®—ä»»åŠ¡ç±»å‹åŒ¹é…åˆ†æ•°"""
        if chunk_task == query_task:
            return 1.0
        
        # ä»»åŠ¡ç±»å‹ç›¸ä¼¼æ€§çŸ©é˜µ
        similarity_matrix = {
            (TaskType.TECHNICAL_CODING, TaskType.EDUCATIONAL): 0.6,
            (TaskType.EMOTIONAL_COUNSELING, TaskType.GENERAL_QA): 0.4,
            (TaskType.CREATIVE_WRITING, TaskType.EDUCATIONAL): 0.3,
        }
        
        # å¯¹ç§°æ€§
        pair = (chunk_task, query_task)
        reverse_pair = (query_task, chunk_task)
        
        return similarity_matrix.get(pair, similarity_matrix.get(reverse_pair, 0.2))
    
    def _calculate_keyword_overlap(self, chunk: MemoryChunk, query: str) -> float:
        """è®¡ç®—å…³é”®è¯é‡å åº¦"""
        # ä»å…ƒæ•°æ®è·å–å…³é”®è¯
        chunk_keywords = set()
        if 'keywords' in chunk.metadata:
            chunk_keywords.update(chunk.metadata['keywords'])
        
        # ä»å†…å®¹æå–å…³é”®è¯
        content_words = set(chunk.content.lower().split())
        chunk_keywords.update(content_words)
        
        # æŸ¥è¯¢å…³é”®è¯
        query_words = set(query.lower().split())
        
        if not chunk_keywords or not query_words:
            return 0.0
        
        # è®¡ç®—é‡å åº¦
        overlap = len(chunk_keywords.intersection(query_words))
        total = len(chunk_keywords.union(query_words))
        
        return overlap / total if total > 0 else 0.0
    
    def _calculate_context_relevance(self, chunk: MemoryChunk, query: str) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå†…å®¹é•¿åº¦å’ŒæŸ¥è¯¢åŒ¹é…åº¦
        content_length = len(chunk.content)
        query_length = len(query)
        
        # é•¿åº¦ç›¸ä¼¼æ€§
        length_similarity = 1.0 - abs(content_length - query_length) / max(content_length, query_length, 1)
        
        # å†…å®¹åŒ…å«æ€§
        query_lower = query.lower()
        content_lower = chunk.content.lower()
        containment = 1.0 if query_lower in content_lower or content_lower in query_lower else 0.5
        
        return (length_similarity * 0.3 + containment * 0.7)
    
    def _assess_content_structure(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹ç»“æ„è´¨é‡"""
        if not content:
            return 0.0
        
        # ç®€å•çš„ç»“æ„æŒ‡æ ‡
        has_sentences = '.' in content or '?' in content or '!' in content
        has_paragraphs = '\n' in content
        reasonable_length = 10 <= len(content) <= 1000
        
        structure_score = (
            0.4 * has_sentences +
            0.3 * has_paragraphs + 
            0.3 * reasonable_length
        )
        
        return structure_score
    
    def _assess_informativeness(self, content: str) -> float:
        """è¯„ä¼°ä¿¡æ¯ä¸°å¯Œåº¦"""
        if not content:
            return 0.0
        
        # ä¿¡æ¯ä¸°å¯Œåº¦æŒ‡æ ‡
        word_count = len(content.split())
        unique_words = len(set(content.lower().split()))
        
        # è¯æ±‡å¤šæ ·æ€§
        lexical_diversity = unique_words / max(word_count, 1)
        
        # åŒ…å«å…·ä½“ä¿¡æ¯çš„æŒ‡æ ‡è¯
        info_indicators = ['å…·ä½“', 'è¯¦ç»†', 'ä¾‹å¦‚', 'æ¯”å¦‚', 'æ­¥éª¤', 'æ–¹æ³•', 'åŸå› ', 'ç»“æœ']
        info_richness = sum(1 for indicator in info_indicators if indicator in content) / len(info_indicators)
        
        return (lexical_diversity * 0.6 + info_richness * 0.4)
    
    def _assess_clarity(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹æ¸…æ™°åº¦"""
        if not content:
            return 0.0
        
        # æ¸…æ™°åº¦æŒ‡æ ‡
        sentences = content.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
        
        # ç†æƒ³å¥é•¿åœ¨10-20è¯ä¹‹é—´
        sentence_length_score = 1.0 - abs(avg_sentence_length - 15) / 15
        sentence_length_score = max(0.0, min(1.0, sentence_length_score))
        
        # é¿å…è¿‡å¤šé‡å¤è¯æ±‡
        words = content.lower().split()
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        max_freq = max(word_freq.values()) if word_freq else 1
        repetition_penalty = min(max_freq / len(words), 0.3)
        
        clarity_score = sentence_length_score * (1.0 - repetition_penalty)
        
        return max(0.0, min(1.0, clarity_score))
    
    def get_scoring_stats(self) -> Dict[str, Any]:
        """è·å–è¯„åˆ†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'weights': dict(self.weights),
            'task_type_weights': {k.value: v for k, v in self.task_type_weights.items()},
            'scoring_components': [
                'recency', 'content_quality', 'task_importance', 'metadata_richness'
            ]
        }

