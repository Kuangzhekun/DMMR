# -*- coding: utf-8 -*-
"""
è®¤çŸ¥åˆ†ç±»å™¨ - åŸºäºåŒé‡åŠ å·¥ç†è®ºçš„ä»»åŠ¡ç±»å‹è¯†åˆ«
å®ç°ç³»ç»Ÿ1(ç›´è§‰)å’Œç³»ç»Ÿ2(åˆ†æ)çš„è®¤çŸ¥åˆ‡æ¢æœºåˆ¶
"""
from typing import Dict, List, Optional
from .data_models import TaskType
from .api_wrapper import APIWrapper
from .config import get_config


class CognitiveTriage:
    """è®¤çŸ¥åˆ†ç±»å™¨ï¼Œåˆ¤æ–­ç”¨æˆ·äº¤äº’çš„æ„å›¾ç±»å‹"""
    
    def __init__(self, api_wrapper: APIWrapper = None):
        self.api_wrapper = api_wrapper
        self.config = get_config().triage
        
        # å…³é”®è¯è§„åˆ™åº“
        self.keywords = {
            TaskType.TECHNICAL_CODING: [
                'ä»£ç ', 'bug', 'é”™è¯¯', 'python', 'å‡½æ•°', 'class', 'ç±»', 'ç®—æ³•',
                'æŠ¥é”™', 'è°ƒè¯•', 'æ€§èƒ½', 'API', 'git', 'powershell', 'åº“', 'æ¡†æ¶',
                'def', 'return', 'import', 'from', 'typing', 'List', 'str', 'int',
                'float', 'bool', 'Dict', 'Optional', '->', 'if', 'for', 'while',
                'docker', 'javascript', 'nodejs', 'react', 'vue', 'angular'
            ],
            TaskType.EMOTIONAL_COUNSELING: [
                'æ„Ÿè§‰', 'éš¾è¿‡', 'å†²çª', 'æœ‹å‹', 'å®¶äºº', 'åŒäº‹', 'ç»ç†', 'ä¼¤å¿ƒ',
                'ç„¦è™‘', 'æ²®ä¸§', 'å¼€å¿ƒ', 'å…³ç³»', 'æ„Ÿæƒ…', 'æƒ…ç»ª', 'è§‰å¾—', 'å¿ƒç†',
                'å‹åŠ›', 'çƒ¦æ¼', 'å›°æ‰°', 'æ‹…å¿ƒ', 'å®³æ€•', 'æ„¤æ€’', 'å¤±è½'
            ],
            TaskType.CREATIVE_WRITING: [
                'å†™ä½œ', 'åˆ›ä½œ', 'æ•…äº‹', 'å°è¯´', 'è¯—æ­Œ', 'æ–‡ç« ', 'å‰§æœ¬', 'åˆ›æ„',
                'çµæ„Ÿ', 'æƒ…èŠ‚', 'äººç‰©', 'å¯¹è¯', 'æè¿°', 'ä¿®è¾', 'é£æ ¼'
            ],
            TaskType.EDUCATIONAL: [
                'å­¦ä¹ ', 'æ•™å­¦', 'è¯¾ç¨‹', 'çŸ¥è¯†', 'æ¦‚å¿µ', 'ç†è®º', 'åŸç†', 'è§£é‡Š',
                'å®šä¹‰', 'ä¾‹å­', 'ç»ƒä¹ ', 'ä½œä¸š', 'è€ƒè¯•', 'å¤ä¹ ', 'æ€»ç»“'
            ]
        }
        
        # LLMåˆ†ç±»ç¼“å­˜
        self.llm_cache = {}
        self.llm_fallback_count = 0
        
        # LLMåˆ†ç±»æç¤ºæ¨¡æ¿
        self.llm_prompt_template = """
è¯·å°†ä»¥ä¸‹ç”¨æˆ·è¾“å…¥åˆ†ç±»åˆ°ä»¥ä¸‹ä»»åŠ¡ç±»å‹ä¹‹ä¸€ï¼š

1. TECHNICAL_CODING - ç¼–ç¨‹ã€è°ƒè¯•ã€ä»£ç ç›¸å…³é—®é¢˜
2. EMOTIONAL_COUNSELING - ä¸ªäººæƒ…æ„Ÿã€äººé™…å…³ç³»ã€å¿ƒç†æ”¯æŒ
3. CREATIVE_WRITING - åˆ›æ„å†™ä½œã€æ•…äº‹åˆ›ä½œã€æ–‡å­¦åˆ›ä½œ
4. EDUCATIONAL - å­¦ä¹ ã€æ•™å­¦ã€çŸ¥è¯†è§£é‡Š
5. GENERAL_QA - ä¸€èˆ¬æ€§é—®é¢˜ã€ä¿¡æ¯æŸ¥è¯¢

ç”¨æˆ·è¾“å…¥ï¼š"{text}"

è¯·åªå›ç­”ä»»åŠ¡ç±»å‹åç§°ï¼ˆå¦‚ï¼š"TECHNICAL_CODING"ï¼‰ï¼Œä¸éœ€è¦è§£é‡Šã€‚
""".strip()
        
        mode_desc = "å…³é”®è¯è§„åˆ™"
        if self.config.use_llm_fallback and self.api_wrapper:
            mode_desc += " + LLMå›é€€"
        
        print(f"ğŸ§  è®¤çŸ¥åˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ ({mode_desc})")
    
    def classify(self, text: str) -> TaskType:
        """
        åˆ†ç±»ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡ç±»å‹
        
        Args:
            text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            
        Returns:
            è¯†åˆ«çš„ä»»åŠ¡ç±»å‹
        """
        text_lower = text.lower()
        
        # 1. å…³é”®è¯è§„åˆ™åˆ†ç±»
        scores = self._calculate_keyword_scores(text_lower)
        
        # 2. è®¡ç®—ç½®ä¿¡åº¦
        max_score = max(scores.values()) if scores.values() else 0
        tied_types = [task_type for task_type, score in scores.items() if score == max_score]
        confidence = self._calculate_confidence(max_score, text_lower)
        
        # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦LLMå›é€€
        if self._should_use_llm_fallback(max_score, len(tied_types), confidence, len(text)):
            llm_result = self._llm_classify(text)
            if llm_result:
                print(f"  [è®¤çŸ¥åˆ†ç±»] å…³é”®è¯åˆ†ç±»ç½®ä¿¡åº¦ä½ï¼ŒLLMåˆ¤åˆ«: {llm_result.value}")
                return llm_result
        
        # 4. è¿”å›å…³é”®è¯åˆ†ç±»ç»“æœ
        best_task_type = tied_types[0] if tied_types and max_score > 0 else TaskType.GENERAL_QA
        print(f"  [è®¤çŸ¥åˆ†ç±»] {best_task_type.value} (åˆ†æ•°: {max_score}, ç½®ä¿¡åº¦: {confidence:.2f})")
        return best_task_type
    
    def _calculate_keyword_scores(self, text_lower: str) -> Dict[TaskType, int]:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        scores = {task_type: 0 for task_type in self.keywords}
        
        for task_type, keywords in self.keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    scores[task_type] += 1
        
        return scores
    
    def _calculate_confidence(self, max_score: int, text_lower: str) -> float:
        """è®¡ç®—åˆ†ç±»ç½®ä¿¡åº¦"""
        total_words = len(text_lower.split())
        if total_words == 0:
            return 0.0
        
        # åŸºç¡€ç½®ä¿¡åº¦ï¼šåŒ¹é…å…³é”®è¯æ•°ä¸æ€»è¯æ•°çš„æ¯”ä¾‹
        base_confidence = max_score / max(total_words, 1)
        
        # è°ƒæ•´å› å­
        adjustment = 0.0
        
        # æ–‡æœ¬é•¿åº¦è°ƒæ•´
        if total_words > 20:  # é•¿æ–‡æœ¬
            adjustment += 0.1
        elif total_words < 5:  # çŸ­æ–‡æœ¬
            adjustment -= 0.1
        
        # ç‰¹å®šæ¨¡å¼è°ƒæ•´
        if any(pattern in text_lower for pattern in ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ']):
            adjustment += 0.1
        
        return min(base_confidence + adjustment, 1.0)
    
    def _should_use_llm_fallback(self, max_score: int, tied_count: int, confidence: float, text_length: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦LLMå›é€€"""
        if not self.config.use_llm_fallback or not self.api_wrapper:
            return False
        
        # LLMå›é€€è§¦å‘æ¡ä»¶
        conditions = [
            max_score == 0,  # æ— å…³é”®è¯åŒ¹é…
            tied_count > 1 and max_score > 0,  # å¤šç±»å‹æ‰“å¹³
            confidence < self.config.confidence_threshold,  # ç½®ä¿¡åº¦è¿‡ä½
            text_length > 100 and max_score < 3  # é•¿æ–‡æœ¬ä½†åŒ¹é…å°‘
        ]
        
        return any(conditions)
    
    def _llm_classify(self, text: str) -> Optional[TaskType]:
        """ä½¿ç”¨LLMè¿›è¡Œåˆ†ç±»"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = text[:200]
        if cache_key in self.llm_cache:
            return self.llm_cache[cache_key]
        
        try:
            # è°ƒç”¨LLM
            prompt = self.llm_prompt_template.format(text=text[:500])
            response = self.api_wrapper.generate_text(
                prompt,
                temperature=0.0,
                max_tokens=50
            )
            
            # è§£æå“åº”
            result = self._parse_llm_response(response)
            
            if result:
                # æ›´æ–°ç¼“å­˜
                self._update_llm_cache(cache_key, result)
                self.llm_fallback_count += 1
                return result
                
        except Exception as e:
            print(f"  [è®¤çŸ¥åˆ†ç±»] LLMåˆ†ç±»å¤±è´¥: {e}")
        
        return None
    
    def _parse_llm_response(self, response: str) -> Optional[TaskType]:
        """è§£æLLMå“åº”"""
        response_clean = response.strip().upper()
        
        # ä»»åŠ¡ç±»å‹æ˜ å°„
        type_mapping = {
            'TECHNICAL_CODING': TaskType.TECHNICAL_CODING,
            'EMOTIONAL_COUNSELING': TaskType.EMOTIONAL_COUNSELING,  
            'CREATIVE_WRITING': TaskType.CREATIVE_WRITING,
            'EDUCATIONAL': TaskType.EDUCATIONAL,
            'GENERAL_QA': TaskType.GENERAL_QA
        }
        
        # ç²¾ç¡®åŒ¹é…
        for type_name, task_type in type_mapping.items():
            if type_name in response_clean:
                return task_type
        
        # æ¨¡ç³ŠåŒ¹é…
        if any(word in response_clean for word in ['TECHNICAL', 'CODING', 'PROGRAMMING']):
            return TaskType.TECHNICAL_CODING
        elif any(word in response_clean for word in ['EMOTIONAL', 'COUNSELING', 'FEELING']):
            return TaskType.EMOTIONAL_COUNSELING
        elif any(word in response_clean for word in ['CREATIVE', 'WRITING', 'STORY']):
            return TaskType.CREATIVE_WRITING
        elif any(word in response_clean for word in ['EDUCATIONAL', 'LEARNING', 'TEACHING']):
            return TaskType.EDUCATIONAL
        
        return TaskType.GENERAL_QA
    
    def _update_llm_cache(self, key: str, value: TaskType):
        """æ›´æ–°LLMç¼“å­˜"""
        # ç®€å•çš„LRUç¼“å­˜
        max_cache_size = getattr(self.config, 'cache_max_size', 100)
        
        if len(self.llm_cache) >= max_cache_size:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = next(iter(self.llm_cache))
            del self.llm_cache[oldest_key]
        
        self.llm_cache[key] = value
    
    def get_classification_stats(self) -> Dict:
        """è·å–åˆ†ç±»å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'use_llm_fallback': self.config.use_llm_fallback,
            'api_available': self.api_wrapper is not None,
            'llm_fallback_count': self.llm_fallback_count,
            'cache_size': len(self.llm_cache),
            'confidence_threshold': self.config.confidence_threshold
        }

